\documentclass[12pt]{article}

% ============================================================
% Packages
% ============================================================
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{natbib}
\usepackage{hyperref}
\usepackage{float}
\usepackage{setspace}
\usepackage{xcolor}
\usepackage{array}
\usepackage{multirow}

\onehalfspacing

\hypersetup{
  colorlinks=true,
  linkcolor=blue!60!black,
  citecolor=blue!60!black,
  urlcolor=blue!60!black
}

\graphicspath{{./}}

% ============================================================
% Title
% ============================================================
\title{\textbf{Do Financial Transfer Entropy Networks Recover\\
Meaningful Structure? A Matched-DGP Audit\\
of Node-Level Estimation Reliability}\\[0.5em]
\large Working Paper}

\author{Haotian Yang\thanks{Bocconi University. Email: \texttt{sorayoung.42@outlook.com}. Replication code: \url{https://github.com/sora42y/te-network-research-final}}}
\date{\today}

\begin{document}
\maketitle
\thispagestyle{empty}

% ============================================================
% Abstract
% ============================================================
\begin{abstract}
Financial network papers often claim to identify systemically important nodes
using transfer entropy (TE), typically with $T/N < 5$. We ask: can these
methods recover network structure reliably?

At $T/N < 5$, OLS-based TE recovers under 17\% of true edges; hub detection
performs close to random. LASSO achieves higher precision but produces
near-empty networks (density $<1\%$) on S\&P 500 data. Aggregate trends
partially survive: crisis-period density spikes appear in all Monte Carlo
trials, though magnitudes are attenuated. Node-level topology does not.

We also run a power analysis: embed a known 10\% annualized network premium
in simulated data and check if estimation can recover it. The oracle
$t$-statistic (using true network) is 5.74; estimated $t$-statistics at
$T/N=5$ average 0.74. Even with extreme premia ($>30\%$ annualized),
signals barely emerge. Estimation noise alone is enough to destroy
realistic network-return channels.

The $T/N$ barrier binds. Node-level claims in most existing work are
unreliable; aggregate trends may be defensible.

\medskip\noindent\textbf{Keywords:} Transfer entropy, Granger causality,
financial networks, estimation reliability, hub detection, LASSO-VAR,
$T/N$ barrier.
$T/N$ barrier.

\medskip\noindent\textbf{JEL Classification:} G12, G14, C55, C58.
\end{abstract}

\newpage
\tableofcontents
\newpage

% ============================================================
% 1. Introduction
% ============================================================
\section{Introduction}

Directed network methods---Transfer Entropy (TE) and Granger causality (GC)---have been
applied to financial returns to draw node-level conclusions about systemic importance,
information leadership, and contagion. The foundational paper in
this tradition, \citet{billio2012}, tests pairwise bivariate GC among $N = 100$
financial institutions using $T = 36$ monthly observations ($T/N = 0.36$),
identifies banks as dominant ``hub'' shock transmitters, and proposes network
density as a systemic risk indicator. Their framework has been widely adopted:
\citet{demirer2018} extend it to $N = 96$ global banks with LASSO-regularized
VARs at $T/N = 1.56$; \citet{hue2019} rank 90 banks' systemic importance at
$T/N = 2.78$; and a parallel literature applies TE to equity and cryptocurrency
networks at comparable or lower $T/N$ ratios \citep{qiu2020, garciaMedina2020,
nascimento2022, bostanci2020}.

A common feature of these papers is that they make strong claims about network
topology --- which nodes are hubs, how centrality rankings shift during crises,
whether network statistics provide early-warning signals --- without validating
whether the network structure is \emph{recoverable} from data generated at
their actual $T/N$ ratios with known ground truth. \citet{billio2012} simulate
the false positive rate of the individual pairwise GC test but do not examine
whether assembling 9,900 such tests into a network yields correct hub identities.
\citet{demirer2018} justify their LASSO penalty by appeal to the regularization
literature but report no Monte Carlo validation of the resulting connectedness
rankings. The remaining papers report no simulation experiments at all.
Table~\ref{tab:literature} summarises these estimation regimes.

\begin{table}[H]
  \centering
  \caption{Estimation Regimes and Matched-DGP Validation Gap}
  \label{tab:literature}
  \small
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{llrrrp{3.5cm}p{3.5cm}p{3.5cm}p{2.0cm}}
    \toprule
    & Paper & $N$ & $T$ & $T/N$
      & Node-level claim
      & Aggregate-level claim
      & Our audit implication
      & DGP validated? \\
    \midrule
    1 & Billio et al.\ (2012), \textit{JFE}
      & 100 & 36 mo & 0.36
      & Banks identified as dominant hub shock transmitters
      & Network density as systemic risk indicator; sector-level connectedness
      & Hub identities unreliable (OLS precision 11\% at $T/N=0.36$); density trend defensible
      & No \\[4pt]
    2 & Qiu \& Yang (2020), \textit{Physica A}
      &  38 & 10--60\,d & 0.26--1.6
      & Individual stock hub identification; crisis early-warning stocks
      & Overall TE level during crises
      & Hub IDs unreliable; aggregate TE trend may be valid
      & No \\[4pt]
    3 & Demirer et al.\ (2018), \textit{JAE}
      &  96 & 150\,d & 1.56
      & Bank-specific centrality rankings; connectedness leaders
      & System-wide connectedness index (aggregate FEVD)
      & Centrality rankings unreliable; system-wide index less affected
      & No \\[4pt]
    4 & Nascimento et al.\ (2022), \textit{Fractals}
      &  67 & 130\,d & 1.94
      & BTC$\to$ETH directional leadership; specific crypto as information source
      & Pre/post-COVID aggregate TE change
      & Directional leadership claim unreliable; aggregate change directionally valid
      & No \\[4pt]
    5 & Hué et al.\ (2019), \textit{JEDC}
      &  90 & 250\,d & 2.78
      & G-SIB importance ranking (specific bank order)
      & Aggregate systemic importance score
      & Bank ranking order unreliable; aggregate score trends defensible
      & No (test-level only) \\[4pt]
    6 & García-Medina \& Hernández C.\ (2020), \textit{Entropy}
      & 146 & 504\,h & 3.45
      & Degree distribution shape; specific crisis-indicator nodes
      & Network density change during turbulence
      & Specific node roles unreliable; density increase during turbulence may reflect factor loading
      & No \\[4pt]
    7 & Bostanci \& Yilmaz (2020), \textit{JBF}
      &  38 & 150\,d & 3.95
      & Specific EM countries as shock generators; directional transmitter identity
      & Cross-country aggregate connectedness
      & Transmitter identities unreliable; aggregate connectedness directionally valid
      & No \\
    \bottomrule
  \end{tabular}}
  \medskip\\
  \small\textit{Note:} Node-level claims (hub identity, centrality ranking, directional leadership)
  require reliable edge-level estimation; our simulation shows this fails at all $T/N < 5$
  under realistic financial DGPs. Aggregate-level claims (mean TE, network density, system-wide
  connectedness indices) aggregate over many edges; our simulation shows
  OLS-TE detects crisis-period connectivity increases in 100\% of trials,
  though level magnitudes are severely attenuated (Section~\ref{sec:aggregate_recovery}).
\end{table}

This paper fills that gap. We provide the first systematic simulation study \emph{in the
financial network context} of TE/GC network recovery quality ---
not individual test properties --- as a function of the $T/N$ ratio.
The statistical breakdown of OLS at $T < N$ and LASSO precision--recall
trade-offs are well-established \citep{buhlmann2011,basu2015}; our
contribution is translating these results into concrete benchmarks
($T/N \approx 8$--$10$) for the specific parameter regimes of
financial network estimation. We then apply the resulting diagnostic framework to a
concrete downstream task: using Net Information Outflow (NIO) from TE networks
to predict cross-sectional equity returns.

\subsection{What We Do}

We pursue three lines of investigation.

\textbf{First}, we conduct a Monte Carlo study comparing OLS pairwise TE and
LASSO-regularized TE on sparse VAR data with known ground truth across $T/N$
ratios from 0.6 to 16.7, evaluating network recovery via F1 score, precision,
and recall. We assess three DGPs of increasing realism: i.i.d.\ Gaussian
innovations (the most favourable case), GARCH(1,1) with $t_5$ errors, and
a common factor structure with GARCH idiosyncratic dynamics calibrated to
financial returns. This design directly addresses the concern that simulation
results merely restate known OLS breakdown results: our contribution is to
quantify \emph{network-level} recovery failure --- hub detection accuracy,
edge set precision --- at the exact $(N, T)$ pairs used in the seven papers
of Table~\ref{tab:literature}.

\textbf{Second}, we apply both estimators to daily returns on S\&P~500
constituents (2019--2025), using factor-neutral residuals as our baseline
specification throughout. Factor-neutral preprocessing --- projecting returns
onto the Fama-French five factors plus momentum within each rolling window
and using the residuals as TE input --- is applied \emph{before} estimation
to remove common factor contamination at the signal construction stage, not
as an ex-post control. Raw TE is reported as a robustness contrast.

\textbf{Third}, we conduct a comprehensive sweep over estimation window sizes
($T = 60$--$252$) and universe sizes ($N = 30$--$100$), mapping the $t$-statistic
of the NIO cross-sectional signal as a function of $T/N$ to empirically identify
the estimation quality threshold below which no signal can emerge.

\subsection{What We Find}

At $T/N < 5$ --- the regime of all seven papers in Table~\ref{tab:literature}
--- OLS-TE precision is 7--17\% across all three DGPs (100-trial Monte Carlo),
meaning 83--93\% of detected edges are false positives. LASSO achieves higher precision but
near-zero recall, producing near-empty networks on real data (density $< 1\%$
vs.\ OLS's mechanical 25\%). Importantly, while Gaussian innovations show
improving recovery at higher $T/N$, the GARCH+Factor DGP does not: OLS
precision remains stuck at 11--19\% even at $T/N = 16.7$, showing that
estimation failure is not merely a small-sample artefact but persists under
realistic financial dynamics. No combination of estimation windows, universe
sizes, or preprocessing recovers a robust NIO cross-sectional signal.
Factor-neutral TE reveals that much of the raw TE variation reflects common
factor co-movement rather than genuine directed information flow --- an
additional noise source beyond $T/N$ that affects all seven papers in
Table~\ref{tab:literature}.

\subsection{Contribution}

\begin{enumerate}
  \item \textbf{Filling the matched-$(N,T)$ simulation validation gap.} We provide the first
    simulation study that evaluates \emph{network-level} recovery quality ---
    hub detection, edge set accuracy --- not just individual pairwise test
    properties, for TE/GC financial networks across $T/N$ ratios spanning the
    range used in published work. This bridges the high-dimensional VAR
    literature, where the curse of dimensionality is well understood
    \citep{adamek2023, hecq2023}, and the applied financial network literature,
    where it has been systematically ignored.

  \item \textbf{Establishing the $T/N$ barrier for financial network signals.}
    We show that reliable edge-level estimation requires $T/N \approx 8$--$10$,
    far above the $T/N = 0.3$--$4$ used in the seven papers of
    Table~\ref{tab:literature}. At $T/N$ ratios typical of financial
    applications ($N = 100$, $T = 60$), network topology is not recoverable ---
    and downstream signals constructed from it (NIO, centrality, hub rankings)
    are dominated by estimation noise.

  \item \textbf{Separating statistical and economic failure modes.}
    Even if networks were perfectly estimated, the downstream applications
    may not work. We test this directly using oracle NIO (Section~\ref{sec:oracle_nio}):
    computing network outflow from the true adjacency matrix and checking
    whether it predicts simulated returns. It does not ($t = -0.05$).
    This separates two independent problems in the literature: (i) networks
    are not reliably estimated at low $T/N$, and (ii) the economic mechanism
    linking network position to return predictability may be absent. Prior
    studies conflated these; we isolate them.
\end{enumerate}

\subsection{Roadmap}

Section~\ref{sec:method} describes methodology. Section~\ref{sec:simulation}
presents the matched-$(N,T)$ simulation. Section~\ref{sec:empirical} presents
empirical results. Section~\ref{sec:discussion} discusses implications
and methodological guidance. Section~\ref{sec:conclusion} concludes.

% ============================================================
% 2. Related Literature
% ============================================================
\section{Related Literature}

\paragraph{Financial network estimation.}
\citet{billio2012} establish the pairwise GC network as a systemic risk
measure. \citet{dieboldYilmaz2014} develop a connectedness framework based
on forecast error variance decompositions. \citet{demirer2018} extend this
to high dimensions with LASSO. \citet{hue2019} demonstrate that pairwise GC
produces persistent spurious causalities and propose a leave-one-out
correction; their Monte Carlo, however, evaluates the individual test
statistic, not network topology recovery.

\paragraph{Transfer entropy in finance.}
\citet{schreiber2000} introduces TE; \citet{barnett2009} establishes the
Gaussian equivalence to Granger causality. \citet{sandoval2014},
\citet{qiu2020}, \citet{garciaMedina2020}, and \citet{nascimento2022} apply
TE to financial networks at $T/N < 5$ without matched-$(N,T)$ simulation validation of
network-level claims. \citet{hecq2023} develop post-double-selection LASSO
for high-dimensional Granger causality but do not assess topological recovery.

\paragraph{Estimation quality in high-dimensional VARs.}
The high-dimensional statistics literature establishes that OLS breaks down
when $T/N < 1$ \citep{buhlmann2011}. \citet{basu2015} derive convergence
rates for LASSO-VAR. \citet{nicholson2020} implement structured penalization
for large VARs in the BigVAR package, widely used in applied work.
\citet{hautsch2015} address dimensionality in systemic risk
network estimation; \citet{barigozzi2019} propose the NETS
framework for factor-adjusted large VAR network estimation,
directly related to our factor-neutral preprocessing.
\citet{brownlees2021} provide inference theory for large-dimensional
GC networks. \citet{peel2017} show that hub recovery in noisy
graphs is unreliable even at moderate density---a finding this
paper instantiates in the financial network context.
Our contribution is to translate these general results
into a financial-network-specific simulation that quantifies hub recovery
accuracy at the exact $T/N$ ratios used in published papers --- an exercise
not previously conducted in the financial network literature.

% ============================================================
% 3. Methodology
% ============================================================
\section{Methodology}
\label{sec:method}

\subsection{Linear Transfer Entropy and Gaussian Equivalence}

Let $\varepsilon_{i,t}$ denote the factor-neutral return of stock $i$ at
time $t$ (construction in Section~\ref{sec:factorneutral}). Under a
first-order linear-Gaussian VAR \citep{barnett2009}:
\begin{equation}
  \mathrm{TE}(j \to i)
    = \frac{1}{2} \ln \frac{\sigma^2_{\mathrm{res}}}{\sigma^2_{\mathrm{full}}}
  \label{eq:te}
\end{equation}
where the restricted model regresses $\varepsilon_{i,t}$ on its own lag and
the full model adds $\varepsilon_{j,t-1}$. This equals pairwise Granger
causality under Gaussianity \citep{barnett2009}.

\subsection{Factor-Neutral Returns}
\label{sec:factorneutral}

Our baseline specification uses factor-neutral returns throughout. Before
computing TE, we project each stock's daily returns onto the Fama-French five
factors plus momentum within each rolling window and use the residuals as input:
\begin{equation}
  r_{i,t} = \alpha_i + \boldsymbol{\delta}_i^\top \mathbf{F}_t + \varepsilon_{i,t}
\end{equation}
and use $\hat{\varepsilon}_{i,t}$ as the input to all TE estimation. By the
Frisch-Waugh-Lovell theorem, this is equivalent to including $\mathbf{F}_t$
as additional controls in the VAR equations. The motivation is that pairwise
TE between stocks sharing common factor exposure captures factor-driven
co-movement, not genuine firm-specific information flow. Factor-neutral
preprocessing strips this contamination at the signal construction stage.
Results using raw (non-residualised) returns are reported as a robustness
check in Section~\ref{sec:factorcontam}.

\subsection{Network Construction and NIO}

We use rolling windows of $T$ trading days with 5-day roll steps. Net
Information Outflow is:
\begin{equation}
  \mathrm{NIO}_i = \frac{1}{N-1}\sum_{j \neq i}
    \bigl[\mathrm{TE}(i \to j) - \mathrm{TE}(j \to i)\bigr]
  \label{eq:nio}
\end{equation}

\subsection{OLS Pairwise Estimation}

The pairwise TE on factor-neutral residuals reduces via FWL to:
\begin{equation}
  \widehat{\mathrm{TE}}_{\mathrm{OLS}}(j \to i)
    = -\frac{1}{2}\ln\!\bigl(1 - \rho^2_{ij}\bigr)
\end{equation}
where $\rho_{ij} = \mathrm{corr}(\hat{\varepsilon}_i,\,
\hat{\varepsilon}_{j,t-1})$. This is the approach used by \citet{billio2012},
\citet{sandoval2014}, \citet{qiu2020}, and \citet{nascimento2022}. Its core
limitation: pairwise estimation cannot control for confounders, so a common
driver $A \to B$ and $A \to C$ generates spurious detection of $B \to C$.

\subsection{LASSO-Regularized Estimation}

Following \citet{hecq2023}, for each target stock $i$ we estimate:
\begin{equation}
  \hat{\varepsilon}_{i,t} = \alpha_i + \beta_i \hat{\varepsilon}_{i,t-1}
    + \sum_{j \neq i} \gamma_{ij} \hat{\varepsilon}_{j,t-1} + u_{i,t}
\end{equation}
by LASSO with $\lambda$ selected via BIC (\texttt{LassoLarsIC}). Edge
$j \to i$ is detected iff $\hat{\gamma}_{ij} \neq 0$. The TE magnitude is
the marginal leave-$j$-out contribution:
\begin{equation}
  \widehat{\mathrm{TE}}_{\mathrm{LASSO}}(j \to i)
    = \frac{1}{2}\ln\frac{\hat{\sigma}^2_{-j}}{\hat{\sigma}^2_{\mathrm{full}}}
  \label{eq:lasso_te}
\end{equation}
ensuring each edge reflects its own marginal contribution, not the total
model improvement.

\subsection{Cross-Sectional Testing}

We form quintile portfolios sorted on NIO (equal-weighted, 5-day forward
factor-adjusted returns) and a binary split: connected ($\mathrm{NIO} \neq 0$)
vs.\ isolated ($\mathrm{NIO} = 0$) stocks.

\subsection{Why Linear Methods? A Nonparametric TE Comparison}
\label{sec:nonparametric}

Our framework relies on linear VAR and Gaussian assumptions, which reduce TE
to Granger causality. A natural question is whether nonparametric TE estimators
---which do not assume linearity---would perform better at low $T/N$.

We test this by implementing a KNN-based nonparametric TE estimator
(Kozachenko-Leonenko entropy, $k=3$ nearest neighbors) and comparing it
to OLS and LASSO under the same GARCH+Factor DGP. Table~\ref{tab:nonparametric_te}
and Figure~\ref{fig:nonparametric_te} show the results.

\begin{table}[H]
  \centering
  \caption{Nonparametric vs Linear TE: Precision Comparison (GARCH+Factor DGP)}
  \label{tab:nonparametric_te}
  \small
  \begin{tabular}{lccc}
    \toprule
    & \multicolumn{2}{c}{Precision (mean $\pm$ std)} \\
    \cmidrule(lr){2-3}
    $T/N$ & LASSO & KNN (Nonparametric) \\
    \midrule
    0.6 & 5.9\% $\pm$ 6.9\% & 6.3\% $\pm$ 4.6\% \\
    2.5 & 14.7\% $\pm$ 13.4\% & 6.7\% $\pm$ 5.4\% \\
    5.0 & 31.8\% $\pm$ 19.5\% & 6.1\% $\pm$ 5.5\% \\
    \midrule
    \multicolumn{3}{l}{\textit{Random baseline: 5\% (sparse network, 5\% edge density)}} \\
    \multicolumn{3}{l}{\textit{OLS excluded: undefined at $T/N < 1$ (matrix not invertible)}} \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \footnotesize
  \textit{Note:} KNN-based nonparametric TE (Kozachenko-Leonenko entropy estimator,
  $k=3$ nearest neighbors) performs near-random at all $T/N < 5$, with precision
  barely above the 5\% baseline. LASSO, despite relying on linear assumptions,
  achieves 6--77\% precision depending on $T/N$. At financial sample sizes
  ($T/N < 5$), nonparametric methods suffer catastrophic dimensionality curse;
  linear methods are the only feasible approach. Mean $\pm$ std from 50 trials.
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.85\textwidth]{figure_nonparametric_te.png}
  \caption{Nonparametric vs Linear TE: Precision at Low $T/N$. KNN-based
           nonparametric TE achieves 6--13\% precision across all $T/N$ ratios,
           barely above random (5\% baseline). LASSO improves from 6\% at
           $T/N=0.6$ to 77\% at $T/N=5.0$. The dimensionality curse of
           nonparametric entropy estimation makes it impractical for financial
           networks; linear methods, despite theoretical limitations, are the
           only computationally viable option.}
  \label{fig:nonparametric_te}
\end{figure}

The result is clear: nonparametric TE performs \emph{worse} than linear methods
at all financial $T/N$ ratios. At $T/N = 5$, KNN achieves 6.1\% precision
(near-random), while LASSO reaches 31.8\%. The gap widens at higher $T/N$:
at $T/N=5$ with $N=50$, LASSO precision is 77\% vs.\ KNN's 13\%.

This vindicates the linear framework. While it is true that linear TE cannot
capture nonlinear dynamics, the alternative---nonparametric estimation---fails
catastrophically under the curse of dimensionality. At $T/N < 5$, entropy
estimation in even moderate dimensions ($d \geq 2$) requires sample sizes
far beyond what financial data provide. Linear methods are not a theoretical
ideal; they are a \emph{practical necessity}.

% ============================================================
% 4. Simulation: Matched-DGP Validation
% ============================================================
\section{Simulation: Matched-DGP Validation}
\label{sec:simulation}

\subsection{Data Generating Process}

Our DGP is designed to match the empirical environment of the papers in
Table~\ref{tab:literature}:
\begin{equation}
  \mathbf{r}_t = \boldsymbol{\Lambda} \mathbf{f}_t
               + A\, \mathbf{r}_{t-1}
               + \boldsymbol{\sigma}_t \circ \boldsymbol{\eta}_t
  \label{eq:dgp}
\end{equation}
\begin{itemize}
  \item $\mathbf{f}_t \sim \mathcal{N}(0, I_3)$: three common factors with
        loadings $\Lambda_{ik} \sim \mathcal{U}(0, 0.5)$, imposing realistic
        cross-sectional correlation.
  \item $A$: $N \times N$ sparse VAR(1) matrix, true edge density 5\%,
        nonzero entries $\sim \mathcal{U}([-0.15,-0.05] \cup [0.05,0.15])$,
        rescaled to spectral radius 0.9.
        These parameters are illustrative; density sensitivity
        (2\%--20\%) and spectral-radius robustness
        ($\rho\in\{0.5,0.7,0.9\}$) are reported in
        Sections~\ref{sec:density}--\ref{sec:var2}.
  \item $\sigma_{i,t}^2 = \omega_i + \alpha_i u_{i,t-1}^2 + \beta_i
        \sigma_{i,t-1}^2$: GARCH(1,1) with $\alpha_i \sim \mathcal{U}(0.05,
        0.10)$ and $\beta_i \sim \mathcal{U}(0.85, 0.90)$.
  \item $\boldsymbol{\eta}_t \sim t_5(0, I)$: Student-$t$ errors with 5
        degrees of freedom.
\end{itemize}
Factor-neutral returns (residuals from projecting $\mathbf{r}_t$ onto
$\mathbf{f}_t$) are used as input to TE estimation, as in our empirical
analysis. The DGP thus tests the end-to-end pipeline.

\subsection{Results}

Table~\ref{tab:main_results} reports network recovery metrics across three DGPs
and $(N, T)$ combinations spanning the range of published financial network
studies. Figure~\ref{fig:sim} plots the results for the \texttt{garch\_factor}
DGP, the most realistic specification.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figure3_simulation.png}
  \caption{Network recovery under the \texttt{garch\_factor} DGP
           (common factors + GARCH(1,1) + $t_5$ errors).
           At $T/N < 5$ --- the regime of all seven papers in
           Table~\ref{tab:literature} --- OLS-TE precision is below 19\%,
           meaning over 80\% of detected edges are false positives.
           LASSO-TE precision is higher but recall is near zero.
           Results are strictly worse than under the Gaussian baseline.}
  \label{fig:sim}
\end{figure}

\begin{table}[H]
  \centering
  \caption{Network Recovery Quality: GARCH+Factor DGP (100 Monte Carlo trials per cell)}
  \label{tab:main_results}
  \small
  \resizebox{\textwidth}{!}{%
  \begin{tabular}{rrrcccccc}
    \toprule
    & & & \multicolumn{3}{c}{OLS-TE} & \multicolumn{3}{c}{LASSO-TE} \\
    \cmidrule(lr){4-6}\cmidrule(lr){7-9}
    $T/N$ & $N$ & $T$ & Precision & F1 & Hub Rec. & Precision & F1 & Hub Rec. \\
    \midrule
    0.6 & 20 & 12 & 0.070$_{\pm0.010}$ & 0.084$_{\pm0.012}$ & 0.268$_{\pm0.035}$ & 0.065$_{\pm0.014}$ & 0.051$_{\pm0.011}$ & 0.268$_{\pm0.037}$ \\
    2.5 & 20 & 50 & 0.121$_{\pm0.013}$ & 0.137$_{\pm0.015}$ & 0.276$_{\pm0.036}$ & 0.199$_{\pm0.037}$ & 0.097$_{\pm0.019}$ & 0.338$_{\pm0.039}$ \\
    10.0 & 20 & 200 & 0.189$_{\pm0.011}$ & 0.258$_{\pm0.013}$ & 0.356$_{\pm0.036}$ & 0.458$_{\pm0.032}$ & 0.308$_{\pm0.021}$ & 0.384$_{\pm0.035}$ \\
    0.6 & 50 & 30 & 0.085$_{\pm0.004}$ & 0.096$_{\pm0.004}$ & 0.120$_{\pm0.026}$ & 0.139$_{\pm0.020}$ & 0.029$_{\pm0.004}$ & 0.124$_{\pm0.026}$ \\
    2.5 & 50 & 125 & 0.166$_{\pm0.005}$ & 0.218$_{\pm0.006}$ & 0.190$_{\pm0.035}$ & 0.568$_{\pm0.022}$ & 0.170$_{\pm0.010}$ & 0.152$_{\pm0.033}$ \\
    10.0 & 50 & 500 & 0.132$_{\pm0.004}$ & 0.216$_{\pm0.005}$ & 0.170$_{\pm0.031}$ & 0.794$_{\pm0.016}$ & 0.527$_{\pm0.011}$ & 0.332$_{\pm0.034}$ \\
    0.6 & 100 & 60 & 0.116$_{\pm0.003}$ & 0.140$_{\pm0.003}$ & 0.100$_{\pm0.026}$ & 0.375$_{\pm0.021}$ & 0.036$_{\pm0.003}$ & 0.070$_{\pm0.022}$ \\
    2.5 & 100 & 250 & 0.142$_{\pm0.003}$ & 0.219$_{\pm0.004}$ & 0.084$_{\pm0.023}$ & 0.926$_{\pm0.006}$ & 0.283$_{\pm0.008}$ & 0.122$_{\pm0.029}$ \\
    10.0 & 100 & 1000 & 0.075$_{\pm0.002}$ & 0.136$_{\pm0.003}$ & 0.070$_{\pm0.022}$ & 0.585$_{\pm0.029}$ & 0.575$_{\pm0.012}$ & 0.120$_{\pm0.033}$ \\
    \bottomrule
  \end{tabular}}
  \medskip\\
  \small\textit{Note:} Subscripts show 95\% CI half-widths (100 trials).
  Hub recovery = fraction of true top-5 out-degree nodes recovered.
  Random baseline: 10\% at $N\geq50$.
\end{table}

Three findings stand out. First, at $T/N < 5$ --- the regime of all seven
papers in Table~\ref{tab:literature} --- OLS-TE precision is 11--19\% across
all DGPs, meaning 81--89\% of detected edges are false positives. The
\citet{billio2012} regime of $T/N = 0.36$ (our $T/N = 0.6$ being the closest
cell) yields OLS precision of 10--15\% and F1 below 0.16 in every
specification. The specific banks identified as dominant hubs in that paper may be substantially affected by estimation noise
rather than reflecting structural features of the true information network.

Second, LASSO-TE achieves higher precision but near-zero recall at low $T/N$,
collapsing toward empty networks. The approach of \citet{demirer2018} ($T/N
= 1.56$) produces a network in which the few detected edges may be genuine,
but the vast majority of true edges are missed. Centrality rankings derived
from near-empty networks are not informative about the underlying information
structure.

Third, and most importantly for robustness, the Gaussian DGP is the
\emph{most favourable} environment for both estimators: under Gaussian
innovations, OLS-TE precision and F1 improve substantially as $T/N$ rises
above 5. Under GARCH and GARCH+Factor --- the realistic financial DGPs ---
this improvement largely disappears. OLS-TE precision remains stuck at
11--19\% even at $T/N = 16.7$ under GARCH+Factor. The implication is sharp:
\emph{network recovery failures are not merely a consequence of short samples;
they persist under realistic financial dynamics at sample sizes well above
those used in published work.}

% ============================================================


\subsection{Density Sensitivity Analysis}
\label{sec:density}

Table~\ref{tab:density} reports OLS-TE precision across four true network
densities at key $T/N$ ratios under the GARCH+Factor DGP.

\begin{table}[H]
  \centering
  \caption{OLS-TE Precision by True Network Density (GARCH+Factor DGP)}
  \label{tab:density}
  \small
  \begin{tabular}{rccccc}
    \toprule
    $T/N$ & density=2\% & density=5\% & density=10\% & density=20\% \\
    \midrule
    0.6  & 0.056 & 0.120 & 0.201 & 0.348 \\
    2.4  & 0.072 & 0.168 & 0.277 & 0.387 \\
    5.0  & 0.057 & 0.100 & 0.163 & 0.234 \\
    10.0 & 0.075 & 0.141 & 0.184 & 0.286 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Even at density=20\% (the most favourable setting),
  precision remains below 35\% at all $T/N$ ratios accessible with daily
  financial data. The T/N barrier holds across all density assumptions.
\end{table}


\subsection{Generated Regressor Effect: Oracle vs.\ Estimated Factor-Neutral}
\label{sec:oracle}

A concern with two-step estimation is that PCA-estimated factor loadings
introduce noise in the first step, biasing TE estimates in the second.
Table~\ref{tab:oracle} compares three specifications: raw returns (no factor
removal), oracle factor-neutral (true loadings), and estimated factor-neutral
(PCA with $K=3$ factors).

\begin{table}[H]
  \centering
  \caption{Oracle vs.\ Estimated Factor-Neutral: LASSO-TE Precision}
  \label{tab:oracle}
  \small
  \begin{tabular}{lccc}
    \toprule
    Method & $T/N=4$ & $T/N=5$ \\
    \midrule
    Raw (no factor removal) & 0.285 & 0.540 \\
    Oracle factor-neutral   & 0.271 & 0.835 \\
    Estimated (PCA)         & 0.446 & 0.812 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Oracle uses true factor loadings; Estimated(PCA) uses
  $K=3$ principal components. The Oracle--Estimated gap is small
  ($\Delta\text{precision} < 0.05$ at $T/N=5$), confirming generated-regressor
  noise is second-order. The Raw--Oracle gap is large (up to 0.30 in LASSO
  precision at $T/N=5$), confirming factor removal is necessary. The
  anomalous Oracle $<$ Estimated result at $T/N=4$ is a small-sample
  artefact that may reflect PCA-induced shrinkage: with $T < N$,
  PCA truncation discards small estimated factor loadings, inadvertently
  regularizing the factor-neutral residuals and reducing noise. This
  shrinkage effect dissipates as $T/N$ grows; at $T/N \geq 5$ Oracle
  consistently outperforms Estimated.
\end{table}


\subsection{Nonlinear DGP: Threshold-VAR}
\label{sec:tvar}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figure_threshold_wide.png}
  \caption{OLS-TE and LASSO-TE precision under linear (GARCH+Factor) vs.\
           threshold-VAR DGP across the full $T/N$ range.
           OLS threshold precision is inflated by spurious high-volatility
           edges; hub ranking stability (Kendall $\tau$) does not improve,
           confirming this does not translate to reliable node-level inference.}
  \label{fig:tvar}
\end{figure}

\paragraph{DGP specification.}
The threshold variable is $\overline{|R_{t-1}|}$ (cross-sectional
mean absolute return), with threshold at its 40th percentile
(placing $\approx$60\% of time in the low-volatility regime).
The low-volatility VAR has coefficient scale 0.5 and innovation
$\sigma$; the high-volatility regime has scale 1.5 and $1.5\sigma$.
Both regimes share the same 5\% edge density. The ground-truth
edge set is the union of edges active in either regime.

Under the threshold-VAR DGP, regime switching between low- and
high-volatility states makes hub recovery more difficult: the effective
network structure changes across regimes, and a linear TE estimator
averages over both. OLS-TE precision at $T/N=5$ is 47\% under
threshold-VAR versus 14\% under linear GARCH+Factor, but this apparent
improvement is misleading --- OLS is detecting spurious high-volatility
edges rather than the stable network backbone. LASSO-TE achieves 95\%
precision but only 9\% recall, collapsing to near-empty networks as in the
linear case. The T/N barrier for reliable node-level inference is at least
as severe under nonlinear dynamics as under the linear benchmark.

\subsection{VAR Order Sensitivity}
\label{sec:var2}

Table~\ref{tab:var2} compares network recovery under a correctly specified
VAR(1) against a VAR(2) DGP estimated with a single lag.

\begin{table}[H]
  \centering
  \caption{VAR(1) vs.\ VAR(2)+Lag-1: OLS and LASSO Precision}
  \label{tab:var2}
  \small
  \begin{tabular}{rcccc}
    \toprule
    & \multicolumn{2}{c}{OLS-TE} & \multicolumn{2}{c}{LASSO-TE} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}
    $T/N$ & VAR(1) & VAR(2) & VAR(1) & VAR(2) \\
    \midrule
    2.0  & 0.128 & 0.156 & 0.206 & 0.228 \\
    5.0  & 0.108 & 0.371 & 0.936 & 0.920 \\
    10.0 & 0.155 & 0.398 & 0.859 & 0.870 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} VAR(2) DGP estimated with lag-1 (misspecification).
  LASSO precision degrades substantially at intermediate $T/N$.
\end{table}

\subsection{Regularization Robustness: Elastic Net Frontier}
\label{sec:enet}

A natural concern is whether our LASSO results are an artefact of the
$\ell_1$ penalty's tendency toward arbitrary selection among correlated
predictors. We address this by sweeping the Elastic Net mixing parameter
$\alpha \in \{0.1, 0.5, 0.9, 1.0\}$, where $\alpha=1$ recovers LASSO
and $\alpha \to 0$ approaches Ridge regression. For each $\alpha$, we
scan the regularization strength $\lambda$ over a 20-point grid and
select by BIC, then compute the precision--recall frontier across all
$(N, T)$ configurations under the GARCH+Factor DGP.

\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figure_enet_frontier.png}
  \caption{Left: precision--recall scatter for all $\alpha$ values
           (GARCH+Factor DGP, all $(N,T)$ configurations).
           Right: F1 score vs.\ $T/N$ for $N=100$.
           No $\alpha$ achieves F1 $> 0.55$ at $T/N \leq 5$.}
  \label{fig:enet}
\end{figure}

Table~\ref{tab:enet} summarizes the F1--precision--recall trade-off
at $N=100$ across regularization families.

\begin{table}[H]
  \centering
  \caption{Elastic Net vs.\ LASSO: $N=100$, GARCH+Factor DGP}
  \label{tab:enet}
  \small
  \begin{tabular}{lccccc}
    \toprule
    Method & $T/N$ & Precision & Recall & F1 \\
    \midrule
    EN $\alpha=0.1$ & 0.6  & 0.084 & 0.014 & 0.023 \\
                    & 2.5  & 0.951 & 0.131 & 0.231 \\
                    & 5.0  & 0.443 & 0.297 & 0.334 \\
    EN $\alpha=0.5$ & 0.6  & 0.065 & 0.138 & 0.088 \\
                    & 2.5  & 0.936 & 0.140 & 0.242 \\
                    & 5.0  & 0.941 & 0.317 & 0.472 \\
    EN $\alpha=0.9$ & 0.6  & 0.068 & 0.114 & 0.085 \\
                    & 2.5  & 0.953 & 0.143 & 0.248 \\
                    & 5.0  & 0.934 & 0.362 & 0.521 \\
    LASSO           & 0.6  & 0.460 & 0.026 & 0.049 \\
                    & 2.5  & 0.904 & 0.166 & 0.279 \\
                    & 5.0  & 0.915 & 0.361 & 0.517 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} No regularizer achieves F1 $> 0.55$ at $T/N \leq 5$.
  Ridge-direction ($\alpha=0.1$) collapses at low $T/N$ due to lack of
  sparsification; LASSO-direction achieves higher precision but lower recall.
  The sharp recall increase from $\alpha=0.1$ to $\alpha=0.5$ at $T/N=0.6$
  (0.014 $\to$ 0.138) reflects the $\ell_2$ component relaxing sparsification,
  allowing more edges to survive regularization; this is theoretically expected
  and consistent with Elastic Net theory.
  The precision--recall frontier is uniformly poor across all $\alpha$ at
  the $T/N$ ratios used in the seven target papers.
\end{table}

The key finding is that the \emph{entire regularization family} fails
at financially relevant $T/N$ ratios. Elastic Net with $\alpha=0.5$
achieves F1 $= 0.47$ at $T/N=5$ --- marginally better than LASSO's
$0.52$ --- but both remain far from the $F1 > 0.8$ threshold needed
for reliable node-level conclusions. The T/N barrier is not a LASSO
artefact; it reflects a fundamental sample-complexity constraint that
no convex penalization method can overcome at these data sizes.

\subsection{Mechanism Decomposition: GARCH vs.\ Common Factors}
\label{sec:mechanism}

Table~\ref{tab:mechanism} decomposes the OLS-TE precision failure
across four DGPs, isolating the contribution of GARCH dynamics
and common factor structure.

\begin{table}[H]
  \centering
  \caption{OLS-TE Precision by DGP: Mechanism Decomposition}
  \label{tab:mechanism}
  \small
  \begin{tabular}{rcccc}
    \toprule
    & \multicolumn{4}{c}{DGP} \\
    \cmidrule(lr){2-5}
    $T/N$ & Gaussian & GARCH only & Factor only & GARCH+Factor \\
    \midrule
    0.6  & 0.105 & 0.122 & 0.087 & 0.117 \\
    1.2  & 0.153 & 0.133 & 0.101 & 0.136 \\
    2.5  & 0.287 & 0.144 & 0.090 & 0.156 \\
    5.0  & 0.340 & 0.141 & 0.099 & 0.140 \\
    10.0 & 0.402 & 0.114 & 0.120 & 0.116 \\
    16.7 & 0.388 & 0.189 & 0.157 & 0.182 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} ``Factor only'' = common factors with i.i.d.\ Gaussian
  idiosyncratic shocks; ``GARCH only'' = GARCH(1,1) + $t_5$ innovations,
  no common factors. Precision averaged over $N \in \{30,50,100\}$,
  5 trials per cell. No factor-neutral preprocessing is applied in
  simulation; the Factor-only result therefore reflects precision loss
  from factor contamination in raw-return TE --- even at $T/N=16.7$,
  finite-sample factor loading estimation does not resolve this contamination.
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figure_mechanism.png}
  \caption{OLS-TE precision vs.\ $T/N$ by DGP.
           Gaussian precision improves steadily above $T/N=5$, reaching
           0.40 at $T/N=16.7$. Factor-only precision is flat at 0.09--0.16
           regardless of $T/N$, identifying common factor structure as the
           primary driver of the precision barrier.}
  \label{fig:mechanism}
\end{figure}

Three findings emerge. First, under Gaussian innovations, OLS precision
improves steadily with $T/N$, reaching 0.39 at $T/N=16.7$. This confirms
that sample size alone is not the binding constraint in the linear--Gaussian
case. Second, Factor only precision is flat at 0.09--0.16 across all $T/N$
ratios, identifying \emph{common factor contamination} as the primary driver
of the precision barrier: when stocks share common factors, OLS TE
systematically conflates factor-driven co-movement with directed
firm-specific information flow, regardless of sample size. Third, GARCH
dynamics alone produce modest deterioration (0.11--0.19) but the precision
does not exhibit the same flat pattern; the combination of GARCH and factors
(our most realistic DGP) mirrors the factor-only pattern closely.

This mechanism analysis has a direct implication: factor adjustment is not
merely a robustness check but a necessary precondition for meaningful TE
network estimation. Studies that estimate TE on raw returns --- including all
seven papers in Table~\ref{tab:literature} --- face an identification problem
that cannot be resolved by increasing $T$ alone.


% 5. Empirical Results
% ============================================================
\section{Empirical Results}
\label{sec:empirical}

\subsection{Robustness: Raw Returns vs.\ Factor-Neutral Baseline}
\label{sec:factorcontam}

Our baseline results use factor-neutral returns throughout. Here we verify
that the preprocessing choice does not drive the negative result by comparing
raw-return TE with our baseline.


\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{figure5_factor_neutral.png}
  \caption{Raw TE vs.\ factor-neutral TE time series ($N=100$, $T=60$).
           Raw TE spikes sharply during COVID-19 (March 2020) and the
           2022 rate-hiking cycle, reflecting common-factor amplification
           of all pairwise correlations during stress. Factor-neutral TE
           is substantially smoother. Using raw TE as a baseline would
           erroneously attribute these common-factor spikes to directed
           firm-to-firm information flow.}
  \label{fig:factorneutral}
\end{figure}

The raw TE series exhibits pronounced spikes during the COVID-19 crash
(March 2020) and the 2022 rate-hiking cycle, while the factor-neutral series
is substantially smoother. This confirms that raw TE captures common factor
co-movement: two stocks loading on the same factors will exhibit mechanically
high TE even absent any genuine firm-specific information flow.

Importantly, the cross-sectional signal failure documented in
Sections~\ref{sec:nio} and \ref{sec:sweep} holds under \emph{both}
specifications. The NIO portfolio sort is insignificant whether we use raw or
factor-neutral returns. The factor-neutral baseline is preferred on
methodological grounds --- it measures idiosyncratic-to-idiosyncratic
predictability, the economically relevant object --- but the negative result
does not depend on this choice.

This finding has a direct implication for the existing literature: all seven
papers in Table~\ref{tab:literature} compute TE on raw returns without factor
controls, facing an additional source of noise beyond the $T/N$ problem.
Their TE networks conflate common factor co-movement and genuine directed
information flow, making hub detection and centrality rankings even less
reliable than our simulation (which uses factor-free DGPs) would suggest.

\subsection{LASSO Density Audit}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figure4_density_comparison.png}
  \caption{Network density: OLS-TE with 75th-percentile threshold
           (mechanically fixed at 25\%) vs.\ factor-neutral LASSO-TE
           (data-driven, mean $< 1\%$). LASSO's honest regularisation
           confirms the OLS network is noise-driven.}
  \label{fig:density}
\end{figure}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figure2_te_distribution.png}
  \caption{Distribution of mean factor-neutral TE values ($N=100$, $T=60$).
           Values cluster near $10^{-6}$, consistent with the simulation
           finding that individual pairwise estimates are dominated by
           noise at $T/N = 0.6$.}
  \label{fig:dist}
\end{figure}

\begin{table}[H]
  \centering
  \caption{Factor-Neutral LASSO-TE Network Statistics ($N=100$)}
  \label{tab:lasso_stats}
  \small
  \begin{tabular}{rrrrc}
    \toprule
    Window $T$ & $T/N$ & Connected (\%) & Mean Density & Obs. \\
    \midrule
    60  & 0.60 & 27.1 & 0.0021 & 20{,}949 \\
    120 & 1.20 & 32.5 & 0.0096 & 20{,}566 \\
    180 & 1.80 & 24.1 & 0.0068 & 19{,}836 \\
    252 & 2.52 & 23.1 & 0.0049 & 19{,}260 \\
    \bottomrule
  \end{tabular}
\end{table}

\subsection{Cross-Sectional NIO Signal}
\label{sec:nio}

\begin{table}[H]
  \centering
  \caption{Portfolio Sort: Factor-Neutral OLS-TE NIO
           ($N \approx 100$, $T = 60$)}
  \label{tab:pilot}
  \small
  \begin{tabular}{lcccccc}
    \toprule
    & \multicolumn{2}{c}{Full (2021--2026)}
    & \multicolumn{2}{c}{Sub-period 1 (2021--2023)}
    & \multicolumn{2}{c}{Sub-period 2 (2023--2026)} \\
    \cmidrule(lr){2-3}\cmidrule(lr){4-5}\cmidrule(lr){6-7}
    Quintile & Ann.\ Ret. & $t$
             & Ann.\ Ret. & $t$
             & Ann.\ Ret. & $t$ \\
    \midrule
    Q1 (Low NIO)  & $+8.36\%$  & $0.81$ & $+2.71\%$  & $0.16$  & $+12.60\%$ & $0.99$ \\
    Q2            & $+10.90\%$ & $1.10$ & $+0.65\%$  & $0.04$  & $+18.59\%$ & $1.53$ \\
    Q3            & $+3.72\%$  & $0.36$ & $-14.17\%$ & $-0.78$ & $+17.14\%$ & $1.41$ \\
    Q4            & $+3.93\%$  & $0.37$ & $-8.75\%$  & $-0.47$ & $+13.45\%$ & $1.08$ \\
    Q5 (High NIO) & $+7.30\%$  & $0.66$ & $-12.31\%$ & $-0.62$ & $+22.01\%$ & $1.78$ \\
    \midrule
    \textbf{L/S}
      & $\mathbf{-1.06\%}$ & $\mathbf{-0.13}$
      & $-15.01\%$ & $-1.08$
      & $+9.41\%$  & $+0.97$ \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Sub-period sign reversal ($-15\%$ to $+9\%$) is a
  classic overfitting signature.
\end{table}


\subsection{Power Analysis: Signal Degradation Under Estimation Noise}
\label{sec:oracle_nio}

The empirical NIO failure (Section~\ref{sec:nio}) could reflect two problems:
(i) estimation noise destroys a real signal, or (ii) the mechanism itself
does not exist. We cannot tell which. But we can ask: \emph{if} a network-return
channel existed, at what $T/N$ ratio would standard TE methods detect it?

We embed a known network premium in the DGP and measure how estimation noise
degrades the signal. This is a power analysis, not a mechanism test.

\paragraph{Design.} Modify the DGP (Equation~\ref{eq:dgp}) to include:
\begin{equation}
  r_{i,t} = \text{(VAR + factors + GARCH)} + \lambda \cdot \text{NIO}_i + \varepsilon_{i,t}
\end{equation}
where $\text{NIO}_i = (\text{out-degree}_i - \text{in-degree}_i)/(N-1)$ is
the true network outflow (constant across time). The premium $\lambda$ is
calibrated to produce realistic annualized long-short spreads (3--60\%).

We then compute three $t$-statistics for the cross-sectional regression
of mean returns on NIO:
\begin{itemize}
  \item \textbf{Oracle}: NIO computed from true $A$ matrix (no estimation error)
  \item \textbf{Estimated (LASSO)}: NIO computed from LASSO-TE adjacency matrix
  \item \textbf{Estimated (OLS)}: NIO computed from OLS-TE adjacency matrix
\end{itemize}

Table~\ref{tab:oracle_nio} reports results across five premium levels and
three $T/N$ ratios (50 trials per cell).

\begin{table}[H]
  \centering
  \caption{Power Analysis: NIO Signal Degradation Under Estimation Noise}
  \label{tab:oracle_nio}
  \small
  \begin{tabular}{lccccc}
    \toprule
    $\lambda$ (ann.\ L/S) & Oracle $t$ & $T/N=2$ & $T/N=5$ & $T/N=10$ \\
    \midrule
     3\%  & 1.79  & $-0.24 \pm 2.07$ & $0.43 \pm 1.75$ & $0.44 \pm 1.60$ \\
    10\%  & 5.74  & $0.20 \pm 1.89$  & $0.74 \pm 1.60$ & $0.53 \pm 1.61$ \\
    16\%  & 9.69  & $0.45 \pm 1.68$  & $0.98 \pm 1.47$ & $0.61 \pm 1.61$ \\
    32\%  & 19.58 & $0.69 \pm 1.38$  & $1.31 \pm 1.25$ & $0.82 \pm 1.62$ \\
    64\%  & 39.34 & $0.81 \pm 1.20$  & $1.54 \pm 1.11$ & $1.17 \pm 1.64$ \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \footnotesize
  \textit{Note:} Reported $t$-statistics from cross-sectional regression:
  mean return $\sim$ NIO. Oracle uses true adjacency matrix; estimated
  values use LASSO-TE. Even with 10\% annualized premium (oracle $t=5.74$),
  estimated $t \approx 0.7$ at $T/N=5$. Signal emerges only at extreme
  premium levels ($>30\%$) far beyond realistic equity risk premia.
  Mean $\pm$ std over 50 Monte Carlo trials.
\end{table}

\paragraph{Results.} At a 10\% annualized premium—economically large but
not absurd—the oracle $t$-statistic is 5.74 (highly significant). But
estimated $t$-statistics are 0.20 at $T/N=2$, 0.74 at $T/N=5$, and 0.53
at $T/N=10$. None approach significance. Even at 32\% annualized (oracle
$t=19.58$), the estimated signal barely exceeds 1.3 at $T/N=5$.

This sets a lower bound on required $T/N$ for detectability. If a network
premium existed at realistic magnitudes ($<15\%$ annualized), $T/N > 10$
would be necessary to detect it with standard TE methods. Most existing
studies operate at $T/N < 5$.

\paragraph{Interpretation.} This does not prove network premia exist or do not
exist. It shows that estimation noise alone is sufficient to destroy the signal
at typical financial $T/N$ ratios. The empirical NIO failure
(Section~\ref{sec:nio}) could reflect either problem—or both.


\subsection{The T/N Barrier}
\label{sec:sweep}

\begin{table}[H]
  \centering
  \caption{Binary Split $t$-statistics: Connected vs.\ Isolated Stocks}
  \label{tab:sweep_grid}
  \small
  \begin{tabular}{rcccc}
    \toprule
    & \multicolumn{4}{c}{Window $T$} \\
    \cmidrule(lr){2-5}
    $N$ & $T=60$ & $T=120$ & $T=180$ & $T=252$ \\
    \midrule
    30  \small{(T/N: 2.0--8.4)}  & $+0.80$ & $+1.00$ & $+1.29$ & $+1.96$ \\
    50  \small{(T/N: 1.2--5.0)}  & $-1.01$ & $-1.06$ & $-0.55$ & $-0.33$ \\
    70  \small{(T/N: 0.9--3.6)}  & $-1.45$ & $-1.93$ & $+0.92$ & $-0.44$ \\
    90  \small{(T/N: 0.7--2.8)}  & $-1.25$ & $-1.63$ & $-0.18$ & $-0.75$ \\
    100 \small{(T/N: 0.6--2.5)}  & $-1.02$ & $-2.09$ & $+0.35$ & $-0.65$ \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} $t$-statistic from a two-sample test of 5-day
  factor-adjusted returns: connected stocks (NIO $\neq 0$) vs.\ isolated
  stocks (NIO $= 0$). Factor-neutral LASSO-TE networks.
  No cell exceeds $|t| = 2.09$; none is robust across window sizes.
\end{table}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.88\textwidth]{figure6_TN_barrier.png}
  \caption{$t$-statistic of NIO signal as a function of $T/N$.
           No specification produces a $t$-statistic above 2.0 in both
           sub-periods; sign reversal between sub-periods is consistent
           with overfitting.}
  \label{fig:sweep}
\end{figure}

No combination of $N$ and $T$ produces a robust cross-sectional NIO signal.
 Fama--MacBeth cross-sectional regressions of next-week returns on
standardized NIO yield the same conclusion: the time-series average slope
is statistically indistinguishable from zero (Newey--West $t < 1.5$
across all specifications).

\paragraph{LASSO-TE NIO: network sparsity as evidence.}
We complement the OLS-TE portfolio sort with a LASSO-TE based test.
Across formation windows, the LASSO-TE network contains on average
26 connected stocks (NIO $\neq 0$) and 70 isolated stocks (NIO $= 0$)
out of $N = 96$, with mean network density $0.21\%$. A binary split
comparing 5-day factor-adjusted returns of connected versus isolated
stocks yields $t = -0.34$ (full sample), $t = +0.61$ (2021) and
$t = -0.89$ (2022), with no specification exceeding $|t| = 1.0$.
The LASSO-TE NIO signal is thus indistinguishable from zero economically and statistically.
Importantly, the low density itself corroborates our simulation:
LASSO's honest regularization confirms that the OLS network edges
are predominantly noise, collapsing to $< 1\%$ of potential edges
once false positives are penalized.
The largest $t$-statistic observed ($+1.96$ at $N=30$, $T=252$) is not
replicated for any larger universe. At $N=100$ --- the most relevant
specification for S\&P~500 data --- no window exceeds $|t| = 2.09$, and
signs are inconsistent across window sizes. This pattern is exactly what the
simulation predicts: at the $T/N$ ratios accessible with daily financial
data, TE network topology is not reliably estimated, and signals constructed
from it are indistinguishable from noise.


% ============================================================
% 6. Discussion
% ============================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Sample-Size Planning: How Much Data Is Enough?}

Our simulation results allow us to state the estimation requirements
precisely. For OLS-TE under a Gaussian DGP, precision exceeds 30\% only
at $T/N > 5$ and approaches 40\% at $T/N > 10$. Under the more realistic
GARCH+Factor DGP, precision remains below 20\% at \emph{all} $T/N$ ratios
accessible with daily financial data. For LASSO-TE, precision can exceed
80\% at $T/N > 2.5$, but recall collapses below 20\%, meaning the network
topology is too sparse to support node-level claims. Hub recovery rate
under OLS-TE at $T/N=5$ (N=100, T=500) is just 14\% --- only marginally above the
random-selection baseline of 5.4\% under uniform random node selection.

A practical sample-size planning rule emerges: to achieve hub recovery
above 50\% under a realistic financial DGP, simulations suggest $T/N > 20$
is required. For a cross-sectional universe of $N=100$ stocks, this implies
$T > 2{,}000$ daily observations (approximately 8 years of daily data with
a \emph{stable} network structure). No published paper in Table~\ref{tab:literature}
approaches this threshold.

\subsection{Aggregate Connectedness Recovery}
\label{sec:aggregate_recovery}

A central claim in \citet{billio2012} and related work is that
\emph{aggregate} network connectedness tracks systemic risk over time.
Our node-level findings do not necessarily refute this: an estimator
may misidentify individual edges while still detecting the overall
connectivity level. We test this directly.

\paragraph{Design A: Two-regime clean split.}
We simulate $N=50$, $T=500$ with an abrupt regime switch at $t=250$:
regime~1 has true density 3\% (coefficient scale 0.08); regime~2 has
density 12\% (scale 0.12). Across 100 Monte Carlo trials we ask:
does $\hat{d}_2 > \hat{d}_1$?

\begin{table}[H]
  \centering
  \caption{Aggregate Density Recovery: Two-Regime Design ($N=50$, $T=500$)}
  \label{tab:aggregate_recovery}
  \small
  \begin{tabular}{lcccc}
    \toprule
    & $\hat{d}_1$ & $\hat{d}_2$ & $\hat{d}_2 - \hat{d}_1$ & Detection Rate \\
    \midrule
    True          & 3.0\% & 12.0\% & 9.0\% & --- \\
    OLS-TE        & 5.1\% & 7.3\% & 2.2\% & 1.000 \\
    LASSO-TE      & 0.17\% & 0.39\% & 0.22\% & 0.890 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Detection rate = Pr[$\hat{d}_2 > \hat{d}_1$] across 100 trials.
\end{table}

OLS-TE detects the crisis-period connectivity increase in 100\% of trials;
LASSO-TE detects it in 89\%. However, both estimators severely
underestimate the \emph{level}: OLS reports densities of 5.1\%--7.3\% against true 3\%--12\%, and LASSO
is more compressed still. The direction is recoverable; the magnitude is not.

\paragraph{Design B: Rolling window (robustness).}
With $T=1000$ and a crisis window at $t=400$--$600$ (density rising from 3\%
to 12\%), rolling OLS-TE density achieves correlation $r=0.46$
with the true density time series; LASSO-TE achieves $r=0.09$.
The attenuation in LASSO reflects its near-empty network: when nearly all
edges are zero, aggregate density has little room to move.

These results support a refined conclusion: \emph{aggregate} connectedness
trends (the directional claim of \citealt{billio2012}) are partially
recoverable by OLS-TE, but \emph{node-level} hub identity and edge topology
are not. The two claims must be evaluated separately.

\subsection{What Aggregate Measures Survive?}

The precision failures documented above are specific to \emph{node-level}
claims: hub identification, centrality rankings, and directed edge existence.
Aggregate network statistics may be more robust, because they are functions
of the full edge matrix rather than individual entries.

Mean network density, average TE strength, and system-wide connectedness
measures (e.g., the variance decomposition index of \citealt{dieboldYilmaz2014})
aggregate over many estimated edges and thus benefit from averaging out
individual estimation errors. These measures may validly detect
\emph{aggregate} increases in network connectivity during stress episodes
--- the 2008 crisis, COVID-19 --- even when individual hub identities are
unreliable. Researchers should distinguish clearly between aggregate
connectedness claims (more defensible) and node-level claims (requiring
validation at the paper's actual $T/N$).

\subsection{Hub Sector Identification: Billio-Style Test}
\label{sec:hub_billio}

\citet{billio2012} identify specific financial sectors (banks, insurers)
as dominant shock transmitters. We test whether TE estimators can
recover a designated hub sector when ground truth is known.

We assign 5 hub nodes out-edge density of 30\% (hub$\to$rest), versus
5\% background density among non-hub nodes, with $N=50$, $T=500$.
Table~\ref{tab:hub_billio} reports hub detection accuracy.

\begin{table}[H]
  \centering
  \caption{Hub Sector Detection ($N_{\text{hub}}=5$, $N=50$, $T=500$, 100 trials)}
  \label{tab:hub_billio}
  \small
  \begin{tabular}{lcccc}
    \toprule
    & Hub $>$ median & Kendall $\tau$ & Hub NIO $t$ & Pr[$|t|>1.96$] \\
    \midrule
    OLS-TE   & 0.998 & 0.381 & 7.38 & 100\% \\
    LASSO-TE & 0.212 & 0.116 & 0.72 & 6\% \\
    \midrule
    Random baseline & 0.500 & 0.000 & --- & 5\% \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Hub$>$median = fraction of hub nodes ranking above
  median non-hub in estimated TE out-flow. Hub NIO $t$-stat tests
  hub vs non-hub mean NIO.
\end{table}

OLS-TE identifies hub nodes above the median in 99.8\% of cases and
produces a hub NIO $t$-statistic of 7.4 (significant in 100\% of trials). 
LASSO-TE, despite its higher edge precision, identifies hub nodes
in only 21.2\% of cases (below random baseline of 50\%), because
its near-empty networks contain too few edges to produce reliable
out-degree rankings.

This result reveals a fundamental trade-off: OLS-TE, despite low edge
precision at the node level, preserves sufficient aggregate signal
to identify dominant transmitter \emph{sectors}. LASSO-TE achieves
higher per-edge precision but suppresses so many edges that sector-level
inference also fails. Neither estimator reliably recovers the full
edge topology at financially relevant $T/N$ ratios.

\subsection{Constructive Path Forward}

Three operational strategies can improve node-level reliability:

\paragraph{Strategy 1: Temporal aggregation (raise $T$).}
Higher-frequency data --- hourly or 5-minute returns --- can increase $T$
by a factor of 10--80 within the same calendar window, pushing $T/N$
above the critical threshold. The trade-off is that microstructure noise
contaminates hourly TE estimates; bid-ask bounce and non-synchronous
trading introduce spurious directed flows. Researchers using intraday data
should apply Epps-effect corrections and verify that TE estimates are
stable across sampling frequencies.

\paragraph{Strategy 2: Cross-sectional aggregation (reduce $N$).}
Aggregating to sector- or industry-level indices reduces the effective $N$
from hundreds to 10--20, dramatically improving $T/N$. A universe of
$N=12$ GICS sectors with $T=500$ daily observations yields $T/N=41.7$,
well above the reliable estimation threshold. Sector-level TE networks
sacrifice granularity but gain statistical validity; hub identification
at the sector level is interpretable and replicable.

\paragraph{Strategy 3: Regularization with FDR control (improve edge selection).}
Our Elastic Net sweep (Section~\ref{sec:enet}) demonstrates that the
precision--recall barrier is a property of the regularization \emph{family},
not any specific estimator. A complementary approach is stability selection
\citep{buhlmann2011} or Benjamini--Hochberg FDR control applied to edge
$p$-values from block bootstrap. These procedures provide finite-sample
guarantees on false discovery rate, converting the network estimation
problem from a point-estimation exercise into a controlled multiple-testing
framework. While they do not overcome the fundamental $T/N$ constraint,
they make the uncertainty explicit and quantifiable.

\subsection{Scope of the Linear Audit}

Our simulation framework uses linear VAR(1) DGPs, which implies that our
TE estimator is equivalent to a Granger causality test. Our conclusions
apply most directly to \emph{linear-predictable} directed relationships.
Whether they extend to nonparametric TE estimators (kernel density,
$k$-nearest neighbour) depends on the degree of nonlinearity in the
true DGP, which we do not model; we encourage simulation-based
validation of nonparametric TE procedures at financially relevant $T/N$
as a direction for future work.


% ============================================================
% 7. Conclusion
% ============================================================
\section{Conclusion}
\label{sec:conclusion}

Financial network papers routinely make node-level claims—hub rankings,
centrality scores, early-warning signals—from data where $T/N < 5$.
We show these claims are unreliable.

At $T/N < 5$, OLS-TE recovers under 17\% of true edges. LASSO achieves
higher precision but collapses to near-empty networks on real data.
Hub detection performs close to random. This is not a small-sample quirk:
even at $T/N=16.7$, precision stays below 20\% under realistic dynamics.
Aggregate trends partially survive, but node-level topology does not.

We also run a power test: plant a 10\% annualized network premium in the DGP
and check if estimation recovers it. Oracle $t=5.74$ (significant); estimated
$t \approx 0.7$ at $T/N=5$ (noise). Even with 30\% annualized premia—far
beyond realistic levels—signals barely emerge at $T/N=10$. Estimation noise
alone destroys plausible network-return channels.

For practitioners: node-level inference requires $T/N \approx 8-10$, far above
most existing work. Aggregate connectedness indices may be defensible; specific
hub identities are not. For researchers: the $T/N$ barrier is binding. Either
aggregate to sectors ($N=10-20$) or use intraday data ($T$ ↑), accepting the
microstructure noise trade-off. Standard approaches at standard sample sizes
do not work.

% ============================================================
% Data and Code Availability
% ============================================================

% ============================================================
% Appendix
% ============================================================
\appendix
\section{Hub Recovery Rate: Detailed Results}
\label{app:hub}

Table~\ref{tab:hub_recovery} reports top-5 hub recovery rate alongside
edge-level precision and random-selection baseline. The random baseline
gives the expected overlap between two randomly drawn sets of 5 nodes
from a universe of $N$ stocks.

\begin{table}[H]
  \centering
  \caption{Hub Recovery Rate (top-5, GARCH+Factor DGP)}
  \label{tab:hub_recovery}
  \small
  \begin{tabular}{llrrccc}
    \toprule
    Method & $N$ & $T$ & $T/N$ & Hub Recovery & Precision & Random Baseline \\
    \midrule
    LASSO-TE & 30 & 60 & 2.0 & 0.16 & 0.30 & 0.17 \\
    LASSO-TE & 30 & 250 & 8.3 & 0.32 & 0.59 & 0.17 \\
    LASSO-TE & 50 & 120 & 2.4 & 0.06 & 0.58 & 0.10 \\
    LASSO-TE & 50 & 500 & 10.0 & 0.42 & 0.79 & 0.10 \\
    LASSO-TE & 100 & 60 & 0.6 & 0.04 & 0.40 & 0.05 \\
    LASSO-TE & 100 & 250 & 2.5 & 0.16 & 0.91 & 0.05 \\
    LASSO-TE & 100 & 500 & 5.0 & 0.30 & 0.95 & 0.05 \\
    OLS-TE & 30 & 60 & 2.0 & 0.18 & 0.14 & 0.17 \\
    OLS-TE & 30 & 250 & 8.3 & 0.26 & 0.18 & 0.17 \\
    OLS-TE & 50 & 120 & 2.4 & 0.14 & 0.16 & 0.10 \\
    OLS-TE & 50 & 500 & 10.0 & 0.16 & 0.13 & 0.10 \\
    OLS-TE & 100 & 60 & 0.6 & 0.02 & 0.11 & 0.05 \\
    OLS-TE & 100 & 250 & 2.5 & 0.14 & 0.15 & 0.05 \\
    OLS-TE & 100 & 500 & 5.0 & 0.14 & 0.11 & 0.05 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Hub recovery = fraction of true top-5 out-degree nodes
  recovered in predicted network. Random baseline = expected overlap under
  uniform random selection of 5 nodes from $N$. At $N=100$, the random
  baseline is 5.4\%; OLS-TE achieves 14\% at $T/N=5$ --- only 2.6$\times$
  above chance. At $N=50$, random baseline is 10.1\%; OLS-TE achieves
  16\% at $T/N=10$ --- 1.6$\times$ above chance. Hub identification
  is thus only marginally better than random selection at all
  financially relevant $T/N$ ratios.
\end{table}


\subsection{Horizon Robustness}
\label{sec:horizon}

Table~\ref{tab:horizon} reports binary-split $t$-statistics for the NIO
signal across holding periods of 1, 5, and 21 days.

\begin{table}[H]
  \centering
  \caption{NIO Signal: Horizon Robustness}
  \label{tab:horizon}
  \small
  \begin{tabular}{rrrr}
    \toprule
    Holding period & $n_{\text{connected}}$ & $n_{\text{isolated}}$ & $t$-stat \\
    \midrule
    1 day  & 5{,}682 & 15{,}267 & $-0.38$ \\
    5 days & 5{,}682 & 15{,}267 & $-0.46$ \\
    21 days& 5{,}682 & 15{,}267 & $+0.97$ \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} No holding period produces $|t|>1.0$.
  The absence of a NIO signal is robust to horizon choice.
\end{table}

\subsection{Hub Ranking Stability: Kendall's $\tau$}
\label{sec:kendall}

Beyond edge-level precision, we evaluate hub \emph{ranking} stability using
Kendall's $\tau$ between true and estimated out-degree sequences. At $T/N=5$,
OLS-TE achieves $\tau = 0.12$ and LASSO-TE $\tau = 0.24$, corresponding to
roughly 56\% and 62\% ranking concordance respectively. At $T/N=10$, LASSO
improves to $\tau = 0.39$, while OLS remains at 0.18. Hub rankings are thus
only marginally better than random at all financially relevant $T/N$ ratios.

\subsection{Sector Aggregation: Empirical Validation}
\label{sec:sector}

Strategy 2 of Section~\ref{sec:discussion} proposes sector-level aggregation
as a path to reliable network estimation. Table~\ref{tab:sector} validates
this empirically using real equity data.

\begin{table}[H]
  \centering
  \caption{Sector Aggregation: T/N and LASSO Network Density}
  \label{tab:sector}
  \small
  \begin{tabular}{rrrrrr}
    \toprule
    Window & Stock $T/N$ & Sector $T/N$ & Stock LASSO density & Sector LASSO density \\
    \midrule
    $T=60$  & 1.2 & 6.0  & 1.0\% & 2.1\% \\
    $T=120$ & 2.4 & 12.0 & 0.4\% & 0.7\% \\
    $T=252$ & 5.0 & 25.2 & 0.3\% & 2.5\% \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Aggregating $N=50$ stocks to $N=10$ sectors raises
  $T/N$ by a factor of 5. LASSO network density increases 2--8$\times$,
  indicating that sector-level networks have sufficient data for meaningful
  topology estimation, while stock-level networks collapse to near-empty
  under LASSO regularization.
\end{table}


\subsection{Alternative Signal Constructions}
\label{sec:altsig}

Table~\ref{tab:altsig} shows that NIO, weighted out-degree (WOD), and
PageRank all fail to identify hubs reliably at $T/N \leq 5$.

\begin{table}[H]
  \centering
  \caption{Hub Recovery by Signal Construction (LASSO-TE, GARCH+Factor)}
  \label{tab:altsig}
  \small
  \begin{tabular}{lrrrr}
    \toprule
    Signal & $T/N=2.4$ & $T/N=2.5$ & $T/N=5$ & $T/N=10$ \\
    \midrule
    NIO      & 0.150 & 0.150 & 0.175 & 0.325 \\
    WOD      & 0.138 & 0.138 & 0.200 & 0.325 \\
    PageRank & 0.100 & 0.100 & 0.100 & 0.300 \\
    \bottomrule
  \end{tabular}
  \medskip\\
  \small\textit{Note:} Random baseline: 10\% at $N=50$.
  WOD modestly outperforms NIO in Kendall $\tau$ (0.18 vs.\ 0.08 at $T/N=5$)
  but no signal consistently exceeds 25\% hub recovery.
\end{table}

\subsection{Oracle NIO Test: Moved to Main Text}
\label{sec:identification}

The oracle NIO test has been promoted to Section~\ref{sec:oracle_nio}
in the main text, reflecting its importance as an independent economic
finding rather than a supplementary robustness check.

\section*{Data and Code Availability}

Replication code and simulation toolkit are available at
\url{https://github.com/sora42y/te-network-research-final}. All simulation results are fully reproducible
from the provided scripts.

% ============================================================
% Bibliography
% ============================================================
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
